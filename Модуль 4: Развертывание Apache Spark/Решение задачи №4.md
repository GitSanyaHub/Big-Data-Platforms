## Использование Apache Spark под управлением YARN для чтения, трансформации и записи данных
Для ВМ проверить, что версия `Hive` совместима с версией `Spark`, установлены `python` и `ipython`. Убедиться, что кластер запущен


- Стартуем на `jn` через пользователя `hadoop`

```
team@team-6-jn:~$ sudo -i -u hadoop
```
- Активируем виртуальную среду `venv`, если она уже создана, иначе создать через команду `python3 -m venv venv
`
```
hadoop@team-6-jn:~$ source venv/bin/activate
```

- После активации в начале строки командной строки будет отображаться название виртуального окружения

```
(venv) hadoop@team-6-jn:~$
```
- Убедиться, что установлены `pyspark` и `onetl` в окружении `venv`. Иначе установить через команды `pip install pyspark` и `pip install onetl`

- Запускаем интерактивную оболочку `IPython`
```
(venv) hadoop@team-6-jn:~$ ipython3
```

- Импортируем модули

```
In [1]: from pyspark.sql import SparkSession
In [1]: from onetl.connection import SparkHDFS
In [2]: from onetl.file import FileDFReader
In [3]: from onetl.file.format import CSV
In [6]: from pyspark.sql import functions as F
In [12]: from onetl.db import DBWriter
In [13]: from onetl.connection import Hive
```
- Делаем инициализацию сессии. 
- Объект `spark` будет точкой входа для работы с кластерализованным `Spark`, 
  который поддерживает `Spark SQL` и взаимодействие с `Hive`. 
  Управление будет через `YARN`

````
In [5]: spark = SparkSession.builder \
   ...:     .master("yarn") \
   ...:     .appName("spark-with-yarn") \
   ...:     .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
   ...:     .config("spark.hive.metastore.uris", "thrift://tmpl-dn-01:9083") \
   ...:     .enableHiveSupport() \
   ...:     .getOrCreate()
````
- Создание объекта `hdfs` для подключения к файловой системе
```
In [6]: hdfs = SparkHDFS(host="tmpl-nn", port=9000, spark=spark, cluster="test")
```

- Сделаем проверку соединения 
```
In [7]: hdfs.check()
```
- Ответ на предыдущую команду будет иметь следующий вид. 
  Это подтверждает, что удалось подключиться к `nn` и получить оттуда информацию
```
SparkHDFS(cluster='test', host='tmpl-nn', ipc_port=9000)
```
- Определяем объект для чтения файлов из папки `/input`, указав разделители в файлах и что в них есть заголовки

```
In [1]: reader = FileDFReader(connection=hdfs, format=CSV(delimiter=",", header=True), source_path="/input")
```

- Прочитаем конкретный файл в переменную
```
In [2]: df = reader.run(["data-20241101-structure-20180828.csv"])
```
- Смотрим количество строк в прочитанном файле
```
In [5]:  df.count()
```
- Смотрим структуру файла
```
In [3]: df.printSchema()
```
 
- Сохраняем данные из определенной колонки, например, `registration date` и просматриваем содержимое
```
In [4]: dt = df.select("registration date")
In [5]: dt.show()
```
- Добавим новую колонку в исходный `df`, которая будет содержать только год из колонки  `registration date`
```
In [7]: df = df.withColumn("reg_year", F.col("registration date").substr(0, 4))
```
- Посмотрим, на сколько партиций разделен исходный файл
```
In [8]: df.rdd.getNumPartitions()
```

- Разделим `df` на `90` партиций согласно колонке `reg_year`
```
In [9]: df = df.repartition(90, "reg_year")
```

- Проверим, что теперь у нас действительно `90` партиций для текущего файла

```
In [10]: df.rdd.getNumPartitions()
```

- Создаем объект `hive` для подключения к `Hive`
```
In [1]: hive = Hive(spark=spark, cluster="test")
```

- Создаем объект `writer` для записи файла по партициям
```
In [2]: writer = DBWriter(
   ...:     connection=hive,
   ...:     table="test.regs",
   ...:     options={
   ...:         "if_exists": "replace_entire_table",
   ...:         "partitionBy": "reg_year"
   ...:     }
   ...: )
```

- Производим запись в `Hive`
```
In [3]: writer.run(df)
```

- Завершаем сессию, чтобы освободить ресурсы
```
In [4]: spark.stop()
```

### Итог: 
- Инициализировали сессию `Spark`
- Прочитали исходные данные через `Spark` под управлением `YARN`
- Применили трансформацию к исходным данным: создали новую колонку с годом регистрации на основе колонки с датой регистрации
- Сделали новое партиционирование файла на основе новой колонки с указанием количества итоговых партиций
- Записали полученные данные 
- Освободили ресурсы
